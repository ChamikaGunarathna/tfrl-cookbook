{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c29a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a0e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_agent import Agent, Brain\n",
    "from envs.policy import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b804296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(agent, env, render=False):\n",
    "    obs, episode_reward, done, step_num = env.reset(),0.0, False, 0\n",
    "    observations, actions = [], []\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        # Save experience\n",
    "        observations.append(np.array(obs).reshape(1, -1))\n",
    "        # Convert to numpy & reshape (8, 8) to (1, 64)\n",
    "        actions.append(action)\n",
    "        episode_reward += reward\n",
    "        obs = next_obs\n",
    "        step_num += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "    env.close()\n",
    "    return observations, actions, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the trajectory rollout method\n",
    "env = GridworldEnv()\n",
    "# input_shape = (env.observation_space.shape[0] *env.observation_space.shape[1], )\n",
    "brain = Brain(env.action_space.n)\n",
    "agent = Agent(env.action_space.n)\n",
    "obs_batch, actions_batch, episode_reward = rollout(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(obs_batch) == len(actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory: (obs_batch, actions_batch, episode_reward)\n",
    "# Rollout 100 episodes; Maximum possible steps = 100 *100 = 10e4\n",
    "trajectories = [rollout(agent, env, render=True) for _ in tqdm(range(100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the reward distribution from a sample of experience data\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sample_ep_rewards = [rollout(agent, env)[-1] for _ in tqdm(range(100))]\n",
    "plt.hist(sample_ep_rewards, bins=10, histtype=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a container for storing trajectories\n",
    "from collections import namedtuple\n",
    "Trajectory = namedtuple(\"Trajectory\", [\"obs\", \"actions\",\"reward\"])\n",
    "# Example for understanding the operations:\n",
    "print(Trajectory(*(1, 2, 3)))\n",
    "# Explanation: `*` unpacks the tuples into individual\n",
    "# values\n",
    "Trajectory(*(1, 2, 3)) == Trajectory(1, 2, 3)\n",
    "# The rollout(...) function returns a tuple of 3 values:\n",
    "# (obs, actions, rewards)\n",
    "# The Trajectory namedtuple can be used to collect\n",
    "# and store mini batch of experience to train the neuro\n",
    "# evolution agent\n",
    "trajectories = [Trajectory(*rollout(agent, env)) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21473aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing elite experiences for the evolution process\n",
    "def gather_elite_xp(trajectories, elitism_criterion):\n",
    "    \"\"\"Gather elite trajectories from the batch of\n",
    "        trajectories\n",
    "    Args:\n",
    "        batch_trajectories (List): List of episode trajectories containing experiences (obs,actions,episode_reward)\n",
    "    Returns:\n",
    "        elite_batch_obs\n",
    "        elite_batch_actions\n",
    "        elite_reard_threshold\n",
    "    \"\"\"\n",
    "    batch_obs, batch_actions,batch_rewards = zip(*trajectories)\n",
    "    reward_threshold = np.percentile(batch_rewards,elitism_criterion)\n",
    "    indices = [index for index, value in enumerate(batch_rewards) if value >= reward_threshold]\n",
    "    elite_batch_obs = [batch_obs[i] for i in indices]\n",
    "    elite_batch_actions = [batch_actions[i] for i in indices]\n",
    "    unpacked_elite_batch_obs = [item for items in elite_batch_obs for item in items]\n",
    "    unpacked_elite_batch_actions = [item for items in elite_batch_actions for item in items]\n",
    "    return np.array(unpacked_elite_batch_obs),np.array(unpacked_elite_batch_actions),reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the elite experience gathering routine\n",
    "elite_obs, elite_actions, reward_threshold = gather_elite_xp(trajectories, elitism_criterion=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccde90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing a helper method to convert discrete action indices to one-hot encoded vectors or probability distribution over actions\n",
    "def gen_action_distribution(action_index, action_dim=5):\n",
    "    action_distribution = np.zeros(action_dim).astype(type(action_index))\n",
    "    action_distribution[action_index] = 1\n",
    "    action_distribution = np.expand_dims(action_distribution, 0)\n",
    "    return action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the action distribution generation function\n",
    "elite_action_distributions = np.array([gen_action_distribution(a.item()) for a in elite_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating and compile the neural network brain with TensorFlow 2.x using the Keras functional API:\n",
    "brain = Brain(env.action_space.n)\n",
    "brain.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the brain training loop\n",
    "elite_obs, elite_action_distributions = elite_obs.astype(\"float16\"), elite_action_distributions.astype(\"float16\")\n",
    "brain.fit(elite_obs, elite_action_distributions, batch_size=128, epochs=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d48867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, brain):\n",
    "        \"\"\"Agent with a neural-network brain powered policy\n",
    "        Args:\n",
    "            brain (keras.Model): Neural Network based model\n",
    "        \"\"\"\n",
    "        self.brain = brain\n",
    "        self.policy = self.policy_mlp\n",
    "        \n",
    "    def policy_mlp(self, observations):\n",
    "        observations = observations.reshape(1, -1)\n",
    "        action_logits = self.brain.process(observations)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n",
    "        return tf.squeeze(action, axis=1)\n",
    "    \n",
    "    def get_action(self, observations):\n",
    "        return self.policy(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, render=True):\n",
    "    obs, episode_reward, done, step_num = env.reset(),0.0, False, 0\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_num += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "    return step_num, episode_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()\n",
    "agent = Agent(brain)\n",
    "for episode in tqdm(range(10)):\n",
    "    steps, episode_reward, done, info = evaluate(agent,env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89151d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters fpr the training loop\n",
    "total_trajectory_rollouts = 70\n",
    "elitism_criterion = 70 # percentile\n",
    "num_epochs = 200\n",
    "mean_rewards = []\n",
    "elite_reward_thresholds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the environment,brain and agent objects\n",
    "env = GridworldEnv()\n",
    "input_shape = (env.observation_space.shape[0] *env.observation_space.shape[1], )\n",
    "brain = Brain(env.action_space.n)\n",
    "brain.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "agent = Agent(brain)\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    trajectories = [Trajectory(*rollout(agent, env)) for _ in range(total_trajectory_rollouts)]\n",
    "    _, _, batch_rewards = zip(*trajectories)\n",
    "    elite_obs, elite_actions, elite_threshold = gather_elite_xp(trajectories,elitism_criterion=elitism_criterion)\n",
    "    elite_action_distributions = np.array([gen_action_distribution(a.item()) for a in elite_actions])\n",
    "    elite_obs, elite_action_distributions = elite_obs.astype(\"float16\"), elite_action_distributions.astype(\"float16\")\n",
    "    brain.fit(elite_obs, elite_action_distributions, batch_size=128, epochs=3, verbose=0);\n",
    "    mean_rewards.append(np.mean(batch_rewards))\n",
    "    elite_reward_thresholds.append(elite_threshold)\n",
    "    print(f\"Episode#:{i + 1} elite-reward-threshold: {elite_reward_thresholds[-1]:.2f} reward:{mean_rewards[-1]:.2f} \")\n",
    "\n",
    "plt.plot(mean_rewards, 'r', label=\"mean_reward\")\n",
    "plt.plot(elite_reward_thresholds, 'g', label=\"elites_reward_threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363405df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
