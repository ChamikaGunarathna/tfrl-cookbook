{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdb288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1946d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Policy Function\n",
    "class PolicyNet(keras.Model):\n",
    "    def __init__(self, action_dim=1):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(24, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(36, activation=\"relu\")\n",
    "        self.fc3 = layers.Dense(action_dim,activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def process(self, observations):\n",
    "        # Process batch observations using `call(x)` behind-the-scenes\n",
    "        action_probabilities = self.predict_on_batch(observations)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5547e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_dim=1):\n",
    "        \"\"\"Agent with a neural-network brain powered policy\n",
    "        Args:\n",
    "            action_dim (int): Action dimension\n",
    "        \"\"\"\n",
    "        self.policy_net = PolicyNet(action_dim=action_dim)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def policy(self, observation):\n",
    "        observation = observation.reshape(1, -1)\n",
    "        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        action_logits = self.policy_net(observation)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n",
    "        return action\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        action = self.policy(observation).numpy()\n",
    "        return action.squeeze()\n",
    "    \n",
    "    def learn(self, states, rewards, actions):\n",
    "        discounted_reward = 0\n",
    "        discounted_rewards = []\n",
    "        rewards.reverse()\n",
    "        \n",
    "        for r in rewards:\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "        discounted_rewards.reverse()\n",
    "            \n",
    "        for state, reward, action in zip(states,discounted_rewards, actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probabilities = self.policy_net(np.array([state]),training=True)\n",
    "                loss = self.loss(action_probabilities, action, reward)\n",
    "            grads = tape.gradient(loss,self.policy_net.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads,self.policy_net.trainable_variables))\n",
    "    \n",
    "    def loss(self, action_probabilities, action, reward):\n",
    "        dist = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * reward\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c326283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n",
    "    \"\"\"Train `agent` in `env` for `episodes`\n",
    "    Args:\n",
    "        agent (Agent): Agent to train\n",
    "        env (gym.Env): Environment to train the agent\n",
    "        episodes (int): Number of episodes to train\n",
    "        render (bool): True=Enable/False=Disable rendering; Default=True\n",
    "    \"\"\"\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                agent.learn(states, rewards, actions)\n",
    "                print(\"\\n\")\n",
    "            print(f\"Episode#:{episode} ep_reward:{total_reward}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:0 ep_reward:-199.0\n",
      "\n",
      "Episode#:1 ep_reward:-199.0\n",
      "\n",
      "Episode#:2 ep_reward:-199.0\n",
      "\n",
      "Episode#:3 ep_reward:-199.0\n",
      "\n",
      "Episode#:4 ep_reward:-199.0\n",
      "\n",
      "Episode#:5 ep_reward:-199.0\n",
      "\n",
      "Episode#:6 ep_reward:-199.0\n",
      "\n",
      "Episode#:7 ep_reward:-199.0\n",
      "\n",
      "Episode#:8 ep_reward:-199.0\n",
      "\n",
      "Episode#:9 ep_reward:-199.0\n",
      "\n",
      "Episode#:10 ep_reward:-199.0\n",
      "\n",
      "Episode#:11 ep_reward:-199.0\n",
      "\n",
      "Episode#:12 ep_reward:-199.0\n",
      "\n",
      "Episode#:13 ep_reward:-199.0\n",
      "\n",
      "Episode#:14 ep_reward:-199.0\n",
      "\n",
      "Episode#:15 ep_reward:-199.0\n",
      "\n",
      "Episode#:16 ep_reward:-199.0\n",
      "\n",
      "Episode#:17 ep_reward:-199.0\n",
      "\n",
      "Episode#:18 ep_reward:-199.0\n",
      "\n",
      "Episode#:19 ep_reward:-199.0\n",
      "\n",
      "Episode#:20 ep_reward:-199.0\n",
      "\n",
      "Episode#:21 ep_reward:-199.0\n",
      "\n",
      "Episode#:22 ep_reward:-199.0\n",
      "\n",
      "Episode#:23 ep_reward:-199.0\n",
      "\n",
      "Episode#:24 ep_reward:-199.0\n",
      "\n",
      "Episode#:25 ep_reward:-199.0\n",
      "\n",
      "Episode#:26 ep_reward:-199.0\n",
      "\n",
      "Episode#:27 ep_reward:-199.0\n",
      "\n",
      "Episode#:28 ep_reward:-199.0\n",
      "\n",
      "Episode#:29 ep_reward:-199.0\n",
      "\n",
      "Episode#:30 ep_reward:-199.0\n",
      "\n",
      "Episode#:31 ep_reward:-199.0\n",
      "\n",
      "Episode#:32 ep_reward:-199.0\n",
      "\n",
      "Episode#:33 ep_reward:-199.0\n",
      "\n",
      "Episode#:34 ep_reward:-199.0\n",
      "\n",
      "Episode#:35 ep_reward:-199.0\n",
      "\n",
      "Episode#:36 ep_reward:-199.0\n",
      "\n",
      "Episode#:37 ep_reward:-199.0\n",
      "\n",
      "Episode#:38 ep_reward:-199.0\n",
      "\n",
      "Episode#:39 ep_reward:-199.0\n",
      "\n",
      "Episode#:40 ep_reward:-199.0\n",
      "\n",
      "Episode#:41 ep_reward:-199.0\n",
      "\n",
      "Episode#:42 ep_reward:-199.0\n",
      "\n",
      "Episode#:43 ep_reward:-199.0\n",
      "\n",
      "Episode#:44 ep_reward:-199.0\n",
      "\n",
      "Episode#:45 ep_reward:-199.0\n",
      "\n",
      "Episode#:46 ep_reward:-199.0\n",
      "\n",
      "Episode#:47 ep_reward:-199.0\n",
      "\n",
      "Episode#:48 ep_reward:-199.0\n",
      "\n",
      "Episode#:49 ep_reward:-199.0\n",
      "\n",
      "Episode#:50 ep_reward:-199.0\n",
      "\n",
      "Episode#:51 ep_reward:-199.0\n",
      "\n",
      "Episode#:52 ep_reward:-199.0\n",
      "\n",
      "Episode#:53 ep_reward:-199.0\n",
      "\n",
      "Episode#:54 ep_reward:-199.0\n",
      "\n",
      "Episode#:55 ep_reward:-199.0\n",
      "\n",
      "Episode#:56 ep_reward:-199.0\n",
      "\n",
      "Episode#:57 ep_reward:-199.0\n",
      "\n",
      "Episode#:58 ep_reward:-199.0\n",
      "\n",
      "Episode#:59 ep_reward:-199.0\n",
      "\n",
      "Episode#:60 ep_reward:-199.0\n",
      "\n",
      "Episode#:61 ep_reward:-199.0\n",
      "\n",
      "Episode#:62 ep_reward:-199.0\n",
      "\n",
      "Episode#:63 ep_reward:-199.0\n",
      "\n",
      "Episode#:64 ep_reward:-199.0\n",
      "\n",
      "Episode#:65 ep_reward:-199.0\n",
      "\n",
      "Episode#:66 ep_reward:-199.0\n",
      "\n",
      "Episode#:67 ep_reward:-199.0\n",
      "\n",
      "Episode#:68 ep_reward:-199.0\n",
      "\n",
      "Episode#:69 ep_reward:-199.0\n",
      "\n",
      "Episode#:70 ep_reward:-199.0\n",
      "\n",
      "Episode#:71 ep_reward:-199.0\n",
      "\n",
      "Episode#:72 ep_reward:-199.0\n",
      "\n",
      "Episode#:73 ep_reward:-199.0\n",
      "\n",
      "Episode#:74 ep_reward:-199.0\n",
      "\n",
      "Episode#:75 ep_reward:-199.0\n",
      "\n",
      "Episode#:76 ep_reward:-199.0\n",
      "\n",
      "Episode#:77 ep_reward:-199.0\n",
      "\n",
      "Episode#:78 ep_reward:-199.0\n",
      "\n",
      "Episode#:79 ep_reward:-199.0\n",
      "\n",
      "Episode#:80 ep_reward:-199.0\n",
      "\n",
      "Episode#:81 ep_reward:-199.0\n",
      "\n",
      "Episode#:82 ep_reward:-199.0\n",
      "\n",
      "Episode#:83 ep_reward:-199.0\n",
      "\n",
      "Episode#:84 ep_reward:-199.0\n",
      "\n",
      "Episode#:85 ep_reward:-199.0\n",
      "\n",
      "Episode#:86 ep_reward:-199.0\n",
      "\n",
      "Episode#:87 ep_reward:-199.0\n",
      "\n",
      "Episode#:88 ep_reward:-199.0\n",
      "\n",
      "Episode#:89 ep_reward:-199.0\n",
      "\n",
      "Episode#:90 ep_reward:-199.0\n",
      "\n",
      "Episode#:91 ep_reward:-199.0\n",
      "\n",
      "Episode#:92 ep_reward:-199.0\n",
      "\n",
      "Episode#:93 ep_reward:-199.0\n",
      "\n",
      "Episode#:94 ep_reward:-199.0\n",
      "\n",
      "Episode#:95 ep_reward:-199.0\n",
      "\n",
      "Episode#:96 ep_reward:-199.0\n",
      "\n",
      "Episode#:97 ep_reward:-199.0\n",
      "\n",
      "Episode#:98 ep_reward:-199.0\n",
      "\n",
      "Episode#:99 ep_reward:-199.0\n",
      "\n",
      "Episode#:100 ep_reward:-199.0\n",
      "\n",
      "Episode#:101 ep_reward:-199.0\n",
      "\n",
      "Episode#:102 ep_reward:-199.0\n",
      "\n",
      "Episode#:103 ep_reward:-199.0\n",
      "\n",
      "Episode#:104 ep_reward:-199.0\n",
      "\n",
      "Episode#:105 ep_reward:-199.0\n",
      "\n",
      "Episode#:106 ep_reward:-199.0\n",
      "\n",
      "Episode#:107 ep_reward:-199.0\n",
      "\n",
      "Episode#:108 ep_reward:-199.0\n",
      "\n",
      "Episode#:109 ep_reward:-199.0\n",
      "\n",
      "Episode#:110 ep_reward:-199.0\n",
      "\n",
      "Episode#:111 ep_reward:-199.0\n",
      "\n",
      "Episode#:112 ep_reward:-199.0\n",
      "\n",
      "Episode#:113 ep_reward:-199.0\n",
      "\n",
      "Episode#:114 ep_reward:-199.0\n",
      "\n",
      "Episode#:115 ep_reward:-199.0\n",
      "\n",
      "Episode#:116 ep_reward:-199.0\n",
      "\n",
      "Episode#:117 ep_reward:-199.0\n",
      "\n",
      "Episode#:118 ep_reward:-199.0\n",
      "\n",
      "Episode#:119 ep_reward:-199.0\n",
      "\n",
      "Episode#:120 ep_reward:-199.0\n",
      "\n",
      "Episode#:121 ep_reward:-199.0\n",
      "\n",
      "Episode#:122 ep_reward:-199.0\n",
      "\n",
      "Episode#:123 ep_reward:-199.0\n",
      "\n",
      "Episode#:124 ep_reward:-199.0\n",
      "\n",
      "Episode#:125 ep_reward:-199.0\n",
      "\n",
      "Episode#:126 ep_reward:-199.0\n",
      "\n",
      "Episode#:127 ep_reward:-199.0\n",
      "\n",
      "Episode#:128 ep_reward:-199.0\n",
      "\n",
      "Episode#:129 ep_reward:-199.0\n",
      "\n",
      "Episode#:130 ep_reward:-199.0\n",
      "\n",
      "Episode#:131 ep_reward:-199.0\n",
      "\n",
      "Episode#:132 ep_reward:-199.0\n",
      "\n",
      "Episode#:133 ep_reward:-199.0\n",
      "\n",
      "Episode#:134 ep_reward:-199.0\n",
      "\n",
      "Episode#:135 ep_reward:-199.0\n",
      "\n",
      "Episode#:136 ep_reward:-199.0\n",
      "\n",
      "Episode#:137 ep_reward:-199.0\n",
      "\n",
      "Episode#:138 ep_reward:-199.0\n",
      "\n",
      "Episode#:139 ep_reward:-199.0\n",
      "\n",
      "Episode#:140 ep_reward:-199.0\n",
      "\n",
      "Episode#:141 ep_reward:-199.0\n",
      "\n",
      "Episode#:142 ep_reward:-199.0\n",
      "\n",
      "Episode#:143 ep_reward:-199.0\n",
      "\n",
      "Episode#:144 ep_reward:-199.0\n",
      "\n",
      "Episode#:145 ep_reward:-199.0\n",
      "\n",
      "Episode#:146 ep_reward:-199.0\n",
      "\n",
      "Episode#:147 ep_reward:-199.0\n",
      "\n",
      "Episode#:148 ep_reward:-199.0\n",
      "\n",
      "Episode#:149 ep_reward:-199.0\n",
      "\n",
      "Episode#:150 ep_reward:-199.0\n",
      "\n",
      "Episode#:151 ep_reward:-199.0\n",
      "\n",
      "Episode#:152 ep_reward:-199.0\n",
      "\n",
      "Episode#:153 ep_reward:-199.0\n",
      "\n",
      "Episode#:154 ep_reward:-199.0\n",
      "\n",
      "Episode#:155 ep_reward:-199.0\n",
      "\n",
      "Episode#:156 ep_reward:-199.0\n",
      "\n",
      "Episode#:157 ep_reward:-199.0\n",
      "\n",
      "Episode#:158 ep_reward:-199.0\n",
      "\n",
      "Episode#:159 ep_reward:-199.0\n",
      "\n",
      "Episode#:160 ep_reward:-199.0\n",
      "\n",
      "Episode#:161 ep_reward:-199.0\n",
      "\n",
      "Episode#:162 ep_reward:-199.0\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    episodes = 2000\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    train(agent, env, episodes)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
